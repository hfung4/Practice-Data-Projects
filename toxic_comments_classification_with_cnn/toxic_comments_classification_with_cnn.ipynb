{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The purpose of this notebook is to examine the performance of neural network models and CNN with word embeddings in multi-class classifications using a small train dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "'''plots'''\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "'''keras'''\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Flatten, Dense, Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, GlobalMaxPooling1D\n",
    "from keras.layers import Dropout,concatenate\n",
    "\n",
    "from keras.layers.core import Reshape\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import regularizers\n",
    "\n",
    "'''Gensim'''\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "\n",
    "'''metrics'''\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw=pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the benefits of using pre-trained word embeddings on problems where there is very little training data available I will only train on 900 samples. I will sample 300 comment_text from each the toxic, obscene, and insult class.  In doing so, I convert this problem from a multi-label classification problem with a large train dataset to a multi-class classification problem with a very small train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function get a subset of the full train/test data'''\n",
    "def get_subset_data(df, size):\n",
    "    df_toxic = df[(df.toxic==1) &\n",
    "       (df.severe_toxic==0) & \n",
    "       (df.obscene==0) & \n",
    "       (df.threat==0) & \n",
    "       (df.insult==0) & \n",
    "       (df.identity_hate==0)]\n",
    "    df_toxic = df_toxic.sample(size)\n",
    "\n",
    "    df_obscene = df[(df.toxic==0) &\n",
    "       (df.severe_toxic==0) & \n",
    "       (df.obscene==1) & \n",
    "       (df.threat==0) & \n",
    "       (df.insult==0) & \n",
    "       (df.identity_hate==0)]\n",
    "    df_obscene = df_obscene.sample(size)\n",
    "\n",
    "    df_insult = df[(df.toxic==0) &\n",
    "       (df.severe_toxic==0) & \n",
    "       (df.obscene==0) & \n",
    "       (df.threat==0) & \n",
    "       (df.insult==1) & \n",
    "       (df.identity_hate==0)]\n",
    "    df_insult= df_insult.sample(size)\n",
    "\n",
    "    data = pd.concat([df_toxic, df_obscene, df_insult], axis=0)\n",
    "    data = data.sample(size*3, random_state=10)\n",
    "    data = data.reset_index(drop=True)\n",
    "\n",
    "    data = data.drop([\"severe_toxic\",\"threat\", \"identity_hate\"], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 5)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = get_subset_data(train_raw, 300)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat the same thing for the test dataset.  I will sample 40 samples from the 3 classes (toxic, obscene, insult) so the test dataset will have 120 samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv('data/test_labels.csv')\n",
    "test_labels = test_labels[test_labels.toxic!=-1]\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_raw = pd.merge(test_data, test_labels, on=\"id\", how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_subset_data(test_raw, 40)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the texts from the train dataset\n",
    "texts = train.comment_text\n",
    "# Get the labels from the train dataset\n",
    "labels = train.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((900,), (900, 3))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text\n",
    "- Use the keras tokenizer to tokenize the text and remove non-alphanumeric symbols and take lowercase of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000 # considers only the top 10000 words in the train dataset\n",
    "maxlen = 100 # cut comments after 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the keras tokenizer, configured to only take into account the 10000 most common words\n",
    "tokenizer = Tokenizer(num_words=max_words,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(texts) # builds the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts) # turn strings into lists of integer indices (one list per doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index # get the word index that the tokenizer computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8448 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(word_index)} unique tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sequences-- lists of integers (word indices) into a 2D integer tensor of shape \n",
    "# (number of train observations, maxlen of each doc)\n",
    "data = pad_sequences(sequences, maxlen= maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2374, 3645, 2375],\n",
       "       [   0,    0,    0, ...,  733,    7,   33],\n",
       "       [   0,    0,    0, ..., 1028, 1454, 1027],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   77,  320, 8439],\n",
       "       [   0,    0,    0, ...,   55,  177,   19],\n",
       "       [8444, 8445,    5, ...,   23, 2064, 2373]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to an array\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data tensor is: (900, 100)\n",
      "\n",
      "The shape of the label tensor is: (900, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the data tensor is: {data.shape}\")\n",
    "print(\"\")\n",
    "print(f\"The shape of the label tensor is: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 2374, 3645, 2375],\n",
       "       [   0,    0,    0, ...,  733,    7,   33],\n",
       "       [   0,    0,    0, ..., 1028, 1454, 1027],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   77,  320, 8439],\n",
       "       [   0,    0,    0, ...,   55,  177,   19],\n",
       "       [8444, 8445,    5, ...,   23, 2064, 2373]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test and train set\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.3, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((630, 100), (270, 100), (270, 100), (270, 3))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the word2vec pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Load KeyedVectors for the Google News word embeddings\n",
    "# Contains 3 million 300-D word embeddings trained from 100 billion words\n",
    "\n",
    "# Load pretrained model (since intermediate data is not included, the model cannot be refined with additional data)\n",
    "# The vectors is loaded from an existing file on disk in the original Google’s word2vec C format as \n",
    "#a KeyedVectors instance\n",
    "word_vectors = KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300.bin.gz', \n",
    "                                                            binary=True) # C bin format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build an Embedding Matrix that I can load to the Embedding layer** \n",
    "- Embedding layer: a dictionary that mpas integer word indices to dense vector representations, it takes a word index, looks up the index in an internal dictionary, and outputs its associated word vectors.\n",
    "- Embedding matrix is a matrix of shape (max_words= 10000, embedding dim = 300).  Each entry i contains a 100-D word vector of the word of index i in the reference word index (built during tokenization).  Note that Index 0 is a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8449, 300)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 300  # dimension of the word embeddings (word vectors)\n",
    "# I will use vocabulary_size to determine the size of the embedding matrix (number of rows)\n",
    "# +1 since index 0 of the embedding matrix is only a placeholder\n",
    "vocabulary_size=min(len(word_index)+1,(max_words)) \n",
    "\n",
    "# Initialize the embedding matrix\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index.items(); # a list of tuples of (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 505 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words: # only get at most 8747 word vectors\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word] # get the word embedding from word2vec\n",
    "        embedding_matrix[i] = embedding_vector # fill the ith row of the embedding matrix with the word vector\n",
    "    except KeyError:  # word not found in word2vec will be zeros\n",
    "        embedding_matrix[i] = np.zeros(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "- Armed with the embedding matrix, I will load it into my embedding layer as its weights (the first layer in my network).\n",
    "- Since the embedding layer uses the weights from the embedding matrix (comes from the pre-trained word2vec), I don't need to update them with backpropogation during training.  I want to \"keep\" my already-learnt features. Thus, I will \"freeze\" the embedding layer.\n",
    "- I will train two fully-connected layers on top of the emebdding layer for multi-class classification (use this as baseline).  The activation function for the last layer should be softmax, and I should use categorical_crossentropy as loss function.\n",
    "- I will also train a CNN for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          2534700   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                960032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 3,494,831\n",
      "Trainable params: 3,494,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# init the network\n",
    "model = Sequential()\n",
    "# embedding layer, I specify the number of possible tokens, the dim of word vectors, \n",
    "# and the length of each doc (sequence)\n",
    "# the output of embedding layer have shape (samples, maxlen, embedding dim) = (900, 100, 300)\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length = maxlen))  \n",
    "# I flatten the 3-D tensor into a 2D tensor of shape (samples, maxlen*300) = (900, 30000)\n",
    "# Each comment/doc is represented by a single 1-D tensor-- each word in the input sequence is treated separately \n",
    "# with no inter-wird relationships and sentence structure\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(32, activation='relu')) # add a hidden layer with 32 neurons\n",
    "# use softmax as activation function for the last layer for multi-class classification (3 classes)\n",
    "model.add(Dense(3, activation= 'softmax')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec pre-trained word vectors to the embedding layer.\n",
    "model.layers[0].set_weights([embedding_matrix])  # embedding layer is the first layer in the network\n",
    "model.layers[0].trainable= False # freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 270 samples\n",
      "Epoch 1/10\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 1.1188 - accuracy: 0.3683 - val_loss: 1.0902 - val_accuracy: 0.3852\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 1s 858us/step - loss: 0.6510 - accuracy: 0.7968 - val_loss: 1.1401 - val_accuracy: 0.3407\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 1s 910us/step - loss: 0.3327 - accuracy: 0.9429 - val_loss: 1.2983 - val_accuracy: 0.3704\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 1s 841us/step - loss: 0.1547 - accuracy: 0.9905 - val_loss: 1.2349 - val_accuracy: 0.4074\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 1s 822us/step - loss: 0.0819 - accuracy: 0.9968 - val_loss: 1.2946 - val_accuracy: 0.3926\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 1s 833us/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 1.5256 - val_accuracy: 0.3852\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 1s 839us/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 1.4681 - val_accuracy: 0.4000\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 1s 858us/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 1.6359 - val_accuracy: 0.3963\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 1s 858us/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 1.8779 - val_accuracy: 0.4037\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 1s 847us/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 1.8761 - val_accuracy: 0.4000\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compile and train the network\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Use early stopping\n",
    "# callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 32,\n",
    "                   validation_data = (X_val, y_val))\n",
    "                   #callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270/270 [==============================] - 0s 155us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8760740368454545, 0.4000000059604645]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val,y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts\n",
    "- Pretty poor performance, but this is to be expected due to the small number of training eamples-- the network will quickly starts overfitting (as seen in plots below).\n",
    "- In addition, my network only consists of several dense layers on top of the embedding layer. The model will not consider inter-word relations and sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot model performance over epochs\n",
    "train_acc = history.history[\"accuracy\"] # train accuracy of each of the 10 train epoch\n",
    "validation_acc = history.history[\"val_accuracy\"]\n",
    "train_loss = history.history[\"loss\"]\n",
    "validation_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1,len(train_acc)+1,1)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,6))\n",
    "axes[0].plot(epochs, train_acc, 'bo', label = \"Training acc\")\n",
    "axes[0].plot(epochs,validation_acc, 'b', label = \"Validation acc\")\n",
    "axes[0].set_title(\"Training and validation accuracy\")\n",
    "axes[0].set_ylabel(\"accuracy\")\n",
    "axes[0].set_xlabel(\"epoch\")\n",
    "\n",
    "\n",
    "axes[1].plot(epochs, train_loss, 'bo', label = \"Training loss\")\n",
    "axes[1].plot(epochs,validation_loss, 'b', label = \"Validation loss\")\n",
    "axes[1].set_title(\"Training and validation loss\")\n",
    "axes[1].set_ylabel(\"loss\")\n",
    "axes[1].set_xlabel(\"epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The validation accuracy basically stalls at 0.4-- it's hard to get good performance with a very small sample!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check performance of model on holdout (test) dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the texts from the test dataset\n",
    "test_texts = test.comment_text\n",
    "# Get the labels from the test dataset\n",
    "test_labels = test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_texts) # turn strings into lists of integer indices (one list per doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sequences-- lists of integers (word indices) into a 2D integer tensor of shape \n",
    "# (number of test observations, maxlen of each doc)\n",
    "X_test = pad_sequences(test_sequences, maxlen= maxlen)\n",
    "y_test = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120, 100), (120, 3))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 192us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0687135537465413, 0.375]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test) # 0.4666 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classifications with convnets\n",
    "- https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/\n",
    "- Convnet convolute across the embedding matrix and extracting features from \"local\" patches of data\n",
    "- As per Kim Yoon's \"Convolutional Neural Networks for Sentence Classification\", we will only use conv layer to extract features using filter_size 1,2 and 3.\n",
    "- The size of the filter is (filter_size, embeding_dim). If filter_size = 1, I will only extract unigram features; and if filter_size=2, I will extract bigrams features.\n",
    "- I will use 1-max pooling to get the max value of the feature map of each filter.\n",
    "- Thus, if I have 6 filters, I will get a 6x1 vector after the 1-max pooling.\n",
    "- Finally, I will have a fully connected layer and softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = X_train.shape[1] # number of words in each doc\n",
    "filter_sizes = [1,2,3] # filter size for conv layer 1, and for conv layer 2  (consider unigrams and bigrams)\n",
    "num_filters = 50 # number of filters for each conv layer\n",
    "drop = 0.4  # probability of dropping neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to the embeddings layer for each doc is bascially an array of word index of length \"sequence_length\",\n",
    "# the length of each doc\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "# Emedding layer:\n",
    "    # Input: word index of shape (100,)\n",
    "    # output: a 3D tensor of shape (samples, sequence_length, embedding_dim) = (samples, 100, 300)\n",
    "embedding = embedding_layer(inputs)\n",
    "# reshape the output of the embedding layer to shape (100, 300, 1 )\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)  \n",
    "\n",
    "# conv layer 1 (use filter window size 1: extract unigrams)\n",
    "# input is the \"reshaped\" embedding matrix\n",
    "conv_0 = Conv2D(num_filters, # number of filters (100) \n",
    "                (filter_sizes[0], embedding_dim), # shape of the filter (1,100)\n",
    "                activation='relu', # activation function\n",
    "                # Regularizers allow you to apply penalties on layer parameters or layer activity during optimization. \n",
    "                # These penalties are summed into the loss function that the network optimizes.\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "\n",
    "# conv layer 2 (use filter window size 2: extract bigrams)\n",
    "# input is the \"reshaped\" embedding matrix\n",
    "conv_1 = Conv2D(num_filters, \n",
    "                (filter_sizes[1], embedding_dim),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "# conv layer 2 (use filter window size 3: extract trigrams)\n",
    "# input is the \"reshaped\" embedding matrix\n",
    "conv_2 = Conv2D(num_filters, \n",
    "                (filter_sizes[2], embedding_dim),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "\n",
    "# Apply max pooling to the activation maps (100 of them) for EACH conv layer\n",
    "# sequence_length - filter_sizes[0] + 1 is the length of Each feature map\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0) # size of 1x100\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "# I get a feature from each of the 1-max pooling layer, so I have 3 features in total\n",
    "# I concat the features to get a 3 feature vector of size 3x100\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1) \n",
    "flatten = Flatten()(merged_tensor) \n",
    "# reshape_1 = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "conc = Dense(20)(dropout)\n",
    "output = Dense(units=3, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 300)     2534700     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 100, 300, 1)  0           embedding_2[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 100, 1, 50)   15050       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 99, 1, 50)    30050       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 98, 1, 50)    45050       reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 50)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 50)     0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 50)     0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 50)     0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 150)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           3020        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            63          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,627,933\n",
      "Trainable params: 93,233\n",
      "Non-trainable params: 2,534,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this creates a model class with takes a Keras.Input object and the outputs of the model as argument\n",
    "model = Model(inputs, output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 270 samples\n",
      "Epoch 1/10\n",
      "630/630 [==============================] - 5s 8ms/step - loss: 1.2068 - accuracy: 0.4000 - val_loss: 1.1328 - val_accuracy: 0.5519\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 1.0578 - accuracy: 0.5730 - val_loss: 1.0830 - val_accuracy: 0.5519\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 0.9579 - accuracy: 0.6794 - val_loss: 1.0646 - val_accuracy: 0.5370\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 0.8811 - accuracy: 0.7333 - val_loss: 1.0858 - val_accuracy: 0.5333\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.8312 - accuracy: 0.7921 - val_loss: 1.0550 - val_accuracy: 0.6037\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 0.7543 - accuracy: 0.8063 - val_loss: 1.1103 - val_accuracy: 0.5407\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 0.7033 - accuracy: 0.8476 - val_loss: 1.0744 - val_accuracy: 0.5481\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 2s 2ms/step - loss: 0.6458 - accuracy: 0.9016 - val_loss: 1.1200 - val_accuracy: 0.5815\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 0.6362 - accuracy: 0.8921 - val_loss: 1.2438 - val_accuracy: 0.5185\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 0.5832 - accuracy: 0.9286 - val_loss: 1.1413 - val_accuracy: 0.5481\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compile and train the network\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Use early stopping\n",
    "# callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 32,\n",
    "                   validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0896392663319905, 0.625]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test) # 0.558 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try 1-D convnet with a slightly different structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 300)          2534700   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 94, 32)            67232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,609,231\n",
      "Trainable params: 2,609,231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# init the network\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length = maxlen))  \n",
    "model.add(Conv1D(32,7,activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(0.01))) # 32 filters, each with filter size of 7\n",
    "model.add(MaxPooling1D(5)) # window size of 5, get max number in each window\n",
    "model.add(Conv1D(32,7,activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01))) # 32 filters, each with filter size of 7\n",
    "model.add(GlobalMaxPooling1D()) # get max number of each filter\n",
    "# use softmax as activation function for the last layer for multi-class classification (3 classes)\n",
    "model.add(Dense(3, activation= 'softmax')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec pre-trained word vectors to the embedding layer.\n",
    "model.layers[0].set_weights([embedding_matrix])  # embedding layer is the first layer in the network\n",
    "model.layers[0].trainable= False # freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 270 samples\n",
      "Epoch 1/10\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 1.8601 - accuracy: 0.3968 - val_loss: 1.7333 - val_accuracy: 0.4185\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 1.5940 - accuracy: 0.5190 - val_loss: 1.5871 - val_accuracy: 0.4185\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 1.4247 - accuracy: 0.6159 - val_loss: 1.5062 - val_accuracy: 0.4000\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 1s 1ms/step - loss: 1.3259 - accuracy: 0.6159 - val_loss: 1.4493 - val_accuracy: 0.4074\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 1.2198 - accuracy: 0.6619 - val_loss: 1.4061 - val_accuracy: 0.4000\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 1.1610 - accuracy: 0.6492 - val_loss: 1.5058 - val_accuracy: 0.3407\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 1.0927 - accuracy: 0.7206 - val_loss: 1.4284 - val_accuracy: 0.4370\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 1.0399 - accuracy: 0.7476 - val_loss: 1.3472 - val_accuracy: 0.4519\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 0.9784 - accuracy: 0.8000 - val_loss: 1.3313 - val_accuracy: 0.4593\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 1s 2ms/step - loss: 0.9643 - accuracy: 0.7762 - val_loss: 1.5000 - val_accuracy: 0.3630\n",
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compile and train the network\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 32,\n",
    "                   validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 383us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4490847667058309, 0.38333332538604736]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test) # 0.47 accuracy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In sum, it is tough to improve the performance of text classification models with small datasets, even with CNN and regularization to alleivate overfitting.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to increase the sample size to 50000\n",
    "- We will also increase the complexity of the problem-- classify comments into 6 non-exclusive classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 8), (63978, 8))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.shape, test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_raw.sample(50000) # take 50000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the texts from the train dataset\n",
    "texts = train.comment_text\n",
    "# Get the labels from the train dataset\n",
    "labels = train.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000,), (50000, 6))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000 # considers only the top 10000 words in the train dataset\n",
    "maxlen = 100 # cut comments after 100 words\n",
    "\n",
    "# init the keras tokenizer, configured to only take into account the 10000 most common words\n",
    "tokenizer = Tokenizer(num_words=max_words,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 102251 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(texts) # builds the word index\n",
    "sequences = tokenizer.texts_to_sequences(texts) # turn strings into lists of integer indices (one list per doc)\n",
    "word_index = tokenizer.word_index # get the word index that the tokenizer computed\n",
    "print(f\"Found {len(word_index)} unique tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sequences-- lists of integers (word indices) into a 2D integer tensor of shape \n",
    "# (number of train observations, maxlen of each doc)\n",
    "data = pad_sequences(sequences, maxlen= maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to an array\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data tensor is: (50000, 100)\n",
      "\n",
      "The shape of the label tensor is: (50000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the data tensor is: {data.shape}\")\n",
    "print(\"\")\n",
    "print(f\"The shape of the label tensor is: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data into test and train set\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, \n",
    "                                                  labels, \n",
    "                                                  test_size=0.3, \n",
    "                                                  shuffle= True,\n",
    "                                                  random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build embedding matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 300  # dimension of the word embeddings (word vectors)\n",
    "# I will use vocabulary_size to determine the size of the embedding matrix (number of rows)\n",
    "# +1 since index 0 of the embedding matrix is only a placeholder\n",
    "vocabulary_size=min(len(word_index)+1,(max_words)) \n",
    "\n",
    "# Initialize the embedding matrix\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words: # only get at most 8747 word vectors\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word] # get the word embedding from word2vec\n",
    "        embedding_matrix[i] = embedding_vector # fill the ith row of the embedding matrix with the word vector\n",
    "    except KeyError:  # word not found in word2vec will be zeros\n",
    "        embedding_matrix[i] = np.zeros(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try 1-D conv net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 300)          3000000   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 94, 32)            67232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 18, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 12, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 3,074,630\n",
      "Trainable params: 3,074,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# init the network\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length = maxlen))  \n",
    "model.add(Conv1D(32,7,activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(0.01))) # 32 filters, each with filter size of 7\n",
    "model.add(MaxPooling1D(5)) # window size of 5, get max number in each window\n",
    "model.add(Conv1D(32,7,activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01))) # 32 filters, each with filter size of 7\n",
    "model.add(GlobalMaxPooling1D()) # get max number of each filter\n",
    "# use softmax as activation function for the last layer for multi-class classification (3 classes)\n",
    "model.add(Dense(6, activation= 'sigmoid')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word2vec pre-trained word vectors to the embedding layer.\n",
    "model.layers[0].set_weights([embedding_matrix])  # embedding layer is the first layer in the network\n",
    "model.layers[0].trainable= False # freeze the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/10\n",
      "35000/35000 [==============================] - 43s 1ms/step - loss: 0.1413 - accuracy: 0.9629 - val_loss: 0.1440 - val_accuracy: 0.9624\n",
      "Epoch 2/10\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.1401 - accuracy: 0.9629 - val_loss: 0.1395 - val_accuracy: 0.9624\n",
      "Epoch 3/10\n",
      "35000/35000 [==============================] - 42s 1ms/step - loss: 0.1389 - accuracy: 0.9629 - val_loss: 0.1400 - val_accuracy: 0.9624\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compile and train the network\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Use early stopping\n",
    "callbacks = [EarlyStopping(monitor='val_loss')]\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                   epochs = 10,\n",
    "                   batch_size = 128,\n",
    "                   validation_data = (X_val, y_val),\n",
    "                   callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the texts from the test dataset\n",
    "test_texts = test.comment_text\n",
    "# Get the labels from the test dataset\n",
    "test_labels = test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_texts) # turn strings into lists of integer indices (one list per doc)\n",
    "\n",
    "# Turn sequences-- lists of integers (word indices) into a 2D integer tensor of shape \n",
    "# (number of test observations, maxlen of each doc)\n",
    "X_test = pad_sequences(test_sequences, maxlen= maxlen)\n",
    "y_test = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63978/63978 [==============================] - 24s 377us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14127448016294986, 0.962231457233429]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thoughts**\n",
    "- Sure enough, by increasing the sample size, even with a relatively simple conv1D model, the test accuracy (on test data) is a lot higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
